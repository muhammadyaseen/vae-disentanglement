- how are $g_i$ instantiated?
- how is the prior instantiated ? 
- If they learn $A$, it should mean that they don't start with the structure (e.g. in DEAR where where they start with a binary adj mat and then learn the weights)

#### Summary
- Generative factors with semantics are not necessarily independent. Instead, there might be an underlying causal structure
- Propose CausalVAE, which includes a Causal Layer to transform independent exogenous factors into causal endogenous ones that correspond to causally related concepts in data
- Causal representations learned by CausalVAE are semantically interpretable, and  their causal relationship as a DAG is identified with good accuracy
- CausalVAE model is able to generate counterfactual data through “do-operation” to the causal factors

#### Section 3 - Causal Disentanglement in VAE
- A Causal Layer, which essentially describes a SCM is introduced to a conventional VAE network. 
- The Causal Layer transforms the independent exogenous factors to causal ndogenous factors corresponding to causally related concepts of interest.
- A mask mechanism is then used to propagate the effect of parental variables to their children, mimicking the assignment operation of SCMs. 
- Such a Causal Layer is the key to supporting intervention or “do-operation”

#### 3.1 - Transforming Ind. Exogen Factors into Causal Reprs
Transforming $\epsilon$ to structured $z$.

In addition to the encoder and the decoder, we introduce a SCM layer to learn causal representations.

Though a general nonlinear SCM is preferred, for simplicity,  
in this work, the Causal Layer exactly implements a Linear SCM: 
$$\textbf{z} = \textbf{A}^T \textbf{z} + \epsilon = (I - \textbf{A}^T)^{-1} \epsilon  \;\;\;\; \epsilon \sim \mathcal{N}(0,I)$$
- $\epsilon$ are the latent exogenous independent vars
- $z$ are the latent endogenous variables with semantics
- We treat both $\epsilon$ and $z$ as latent vars

We adopt additional information $u$ associated with the true causal concepts as  
supervising signal. (Related to: [[DEAR#3 2 - Supervised regularizer]]).

It is used in 2 ways:
- Firstly, we propose a conditional prior $p(z|u)$ to regularize the learned posterior of $z$. This guarantees that the learned model belongs to an identifiable family. 
- Secondly, we also leverage $u$ to learn the causal structure $A$.

### 3.2 - SCM Layer
Once the causal representation $z$ is obtained, it passes through a Mask Layer to reconstruct itself. Note that this step resembles a SCM which depicts how children are  
generated by their corresponding parental variables.
- This layer makes intervention or ”do-operation” possible
- The adj mat $A$ is weighted such that each column $A_i \in \mathbb{R}^k$ and $A_{ji}$ encodes causal strength from $z_j$ to $z_i$.
- They also model a set of mild, invertible non-linear functions $g_1,\ldots,g_k$ that map parental vars to child vars. (Related: [[DEAR#Implementation of the SCM details from Appendix]]). Hence: 
$$z_i = g_i(A_i \odot z; \eta_i) + \epsilon_i$$
- $g_i$ is a non linear function
- $\eta_i$ are the params of $g_i$
- By minimizing the reconstruction error, the adjacency matrix $A$ and the parameter $\eta_i$ of the mild nonlinear function $g_i$ are trained. (Similar to params of transformation $f_2$ in [[DEAR]]?)

### 3.3 - A Probabilistic Generative Model for CausalVAE 

Model is parameterized by $\theta = (f, h, C, T, \lambda)$
- $f(z)$ is the decoder
- $h(x,u)$ is the encoder
- $C = (I - A^T)^{-1}$
- $T(z) := ( \mu(z), \sigma(z))$
- $\lambda = (\lambda_1(u) = u, \lambda_2 = 1)$

Decoding: $x = f(z) + \xi$
Encoding: $\epsilon = h(x,u) + \zeta$

$$p_\theta(x,z,\epsilon|u) = p_\theta(x|z,\epsilon,u) \times p_\theta(\epsilon, z |u)$$
Where they instantiate:
$$ \begin{equation}
p_\theta(\epsilon, z |u)  = p(\epsilon) \times p_\theta(z |u)
\end{equation}
$$
$$ \begin{equation}
p_\theta(\epsilon, z |u)  = \mathcal{N}(0,I) \times \prod_i^n p_\theta(z_i | u_i)
\end{equation}
$$
$$ \begin{equation}
p_\theta(\epsilon, z |u)  = \mathcal{N}(0,I) \times \prod_i^n \mathcal{N}(z_i ;  \lambda_1(u_i), \lambda_2^2(u_i))
\end{equation}
$$
$$ \begin{equation}
p_\theta(\epsilon, z |u)  = \mathcal{N}(0,I) \times \prod_i^n \mathcal{N}(z_i ;  u_i, 1)
\end{equation}
$$
Essentially, somewhat like [[DEAR#3 2 - Supervised regularizer]], they have $\mathbb{E}(z_i|X) = u_i$

### Learning Strategy

The ELBO objective is: (Proof in appendix)

$$ 
\begin{align}
	\mathbb{E}_{q_\mathcal{X}}[\mathbb{E}_{q_\phi(z \vert x,u)} [\log p_\theta(x \vert z)] & \\ - \mathcal{D}(q_\phi(\epsilon \vert x, u) \vert \vert p(\epsilon)) & \\ - \mathcal{D}(q_\phi(z\vert x, u) \vert \vert p_\theta(z \vert u))
\end{align}
$$

How is $q_\phi(z\vert x, u)$ and $q_\phi(\epsilon \vert x, u)$ implemented in the network ?

### 6 - Experiments
- The results shown on the datasets (Pendulum, Flow, CelebA-Smile, CelebA-Beard) consist of 4 concepts.
- I'm not sure how it was actually implemented. When using CelebA-Smile dataset how do they model other latent attrs that they don't supervise ?

We expect that the pattern of the manipulated concept will be fixed across all images under the same intervention. 
- Pendulum: For example, when we intervene the pendulum `ANGLE` the `ANGLE` of pendulum of different images are almost the same. Meanwhile, we also observe that the `SHADOW LOCATION` and `SHADOW LENGTH` change in a correct way that aligns with the physics law.
- CelabA: When we intervene the cause concept `SMILE`, the status of `MOUTH_OPEN` also changes. In contrast, intervening effect concept `MOUTH_OPEN` does not cause the cause concept `SMILE` to change.

(So they don't seem to claim that they can change the cause without effects being affected as well, as I interpreted in [[DEAR#5 1 Causal controllable generation]] because of ambiguous language. When a parent is intervened upon, the children also get affected)












