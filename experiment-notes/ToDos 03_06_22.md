Read Jonas' msg and take pointers

Pointers from Jonas' msg:
	- Find excerpt from Understanding BetaVAE paper [done]
	- Run training to convergence for BVAE [running]
	- What hypothesis are we testing ?
	
Recap results from:
	- $\beta$ -VAE run with $\beta = 1$ on dsprites and its effect on correlations
	- CS-VAE result on dsprites
		- KLD loss for latents lower in the hierarchy is going to zero and reconstruction is very bad
		- Losses seem to be behaving erratically, going up and down.
		- Almost all the $\mu$ components in all latent layers are very close to zero from above (small positive numbers)
		- 
		- It seems that training is stuck, without any improvement. Low capacity ?
		- I think that current architecture doesn't have enough capacity to carry info to decoder. The `DAGInteractionLayer` cause a severe information bottleneck. I should try increasing `interm_unit_dim` , then possibly including more intermediate layers ?
		- In the Bottom Up networks i'm using `SimpleFCNNEncoder` which is a network with only 1 hidden layer, I can also try out increasing its depth.
		- Another question is... I have to somehow make the units relevant to the labels. There is an easy way to do this, but I have to think + implement it. Have to do this by this weekend so that I can go to Jilles with a couple of things and possibly talk about Seminar.
		- Need to think about how it would be implemented - in a general way
 
What is the correct way to visualize $\mu$ components in `train_step_end()` ? 
Right now I'm averaging the the value of dimension k for each example in a batch so I'm plotting $$\mu_k = \frac{1}{B} \sum_{b=1}^B \mu_b[k]$$, but I think it would naturally tend towards 0 ? Should I instead be plotting histogram of mean components per epoch ?


In bottom-up network `SimpleFCNNEncoder` we are using Tanh() activation and then a linear layer, this can output -ve values. Since we use half of the output layer as sigma, this isn't really valid.

- [x] Check how hparams work in tensorboard - Result: Not that difficult, can implement when needed
- [x] Thinking aux classification head impl
- [x] Correct way to visualize $\mu$ components - Result : Added histogram plot
- [x] Add command line params saving


