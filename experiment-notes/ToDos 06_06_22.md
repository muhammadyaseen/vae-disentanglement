

- [x] Plot Latent traversals of highest varying dimensions for $\beta=1$ and epochs=50 network
	- To add to the weirdness... The dims that show no change upon reconstruction are getting mapped to zero. I have tried with several achnor images, they always get mapped to zero or very small values in those dims
	- Dims that show sq difference show no reconstruction difference, and dims that show very small sq difference show large recon difference.

![[sq_diff_vs_feat_imps.png]]
This is also visible when comparing feature imps (dims with high feature imps are dims that have low sq difference)
- [ ] Train another network for 50 or more epochs but with higher $\beta$ e.g. $\beta=2$ or more. Why? More disent pressure?
- [x] Run new high-capacity CSVAE on `DSprites` and interpret results : Results noted below
- [ ] Implement classification heads and think of a more general strategy to localize information in units


- [x] Latent traversal experiment
- [ ] Run expanded / high capacity CSVAE on dsprites for 50 epochs 
	- Didn't help. Main observations are that Recon loss starts at a higher value compared to normal betaVAE architecture (~25 vs  ~140) and the mu activations are all zero.
	- The two phenomenon go together because if activations are zero / non-informative than we of course can't get a good reconstruction.
	- So I should first investigate why all the activations are zero. I had already noticed these zeros when I was verifying structure of CSVAE. I should now look deeper into it
	- 


