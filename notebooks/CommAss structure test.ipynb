{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92a3dec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Saarbrucken\\EDA_Research\\vae-disentanglement\\disentanglement_lib_pl\n"
     ]
    }
   ],
   "source": [
    "cd D:\\Saarbrucken\\EDA_Research\\vae-disentanglement\\disentanglement_lib_pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c54c28bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from architectures import encoders, decoders\n",
    "from common.ops import Flatten3D, Unsqueeze3D, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5d67862",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = getattr(encoders, 'SimpleGaussianConv64CommAss')(10,3,64)\n",
    "dec = getattr(decoders, 'SimpleConv64CommAss')(10,3,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c561a346",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.rand(size=[1,3,64,64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82291afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "enc = nn.Sequential(\n",
    "    nn.Conv2d(1, 5, 2, 1, 0), # B,5,2,2\n",
    "    nn.ReLU(True),\n",
    "    Flatten3D(), # B,5x2x2\n",
    "    nn.Linear(20, 10),\n",
    "    nn.Linear(10, num_latent)\n",
    ")\n",
    "\n",
    "dec = nn.Sequential(\n",
    "    Unsqueeze3D(),\n",
    "    nn.Conv2d(num_latent, 20, 1, 1), \n",
    "    nn.ReLU(True),\n",
    "    Reshape([5, 2, 2]),\n",
    "    nn.ConvTranspose2d(5, 1, 2, 1, 0)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1bd8b82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 3, 3]) tensor([[[[-0.1105, -0.2154, -0.3674],\n",
      "          [-0.1536, -0.5432, -0.3171],\n",
      "          [-0.3573, -0.3726, -0.3949]]]],\n",
      "       grad_fn=<SlowConvTranspose2DBackward>)\n"
     ]
    }
   ],
   "source": [
    "o = Unsqueeze3D()(o)\n",
    "#print(o.shape, o)\n",
    "o = nn.Conv2d(2, 20, 1, 1)(o)\n",
    "o = nn.ReLU(True)(o)\n",
    "#print(o.shape, o)\n",
    "o = Reshape([5,2, 2])(o)\n",
    "#print(o.shape, o)\n",
    "o = nn.ConvTranspose2d(5, 1, 2, 1, 0)(o)\n",
    "print(o.shape, o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7568203f",
   "metadata": {},
   "source": [
    "# Multiscale + GNN structure test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "211a0ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Saarbrucken\\EDA_Research\\vae-disentanglement\\disentanglement_lib_pl\n"
     ]
    }
   ],
   "source": [
    "cd D:\\Saarbrucken\\EDA_Research\\vae-disentanglement\\disentanglement_lib_pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6052ccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from common.ops import Flatten3D, Unsqueeze3D, Reshape\n",
    "\n",
    "class MultiScaleEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder as used in 'Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations'\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim, in_channels, num_nodes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_dim = feature_dim\n",
    "        self.num_nodes = num_nodes\n",
    "        self.features_to_take = self.feature_dim // self.num_nodes\n",
    "        self.batch_size = None\n",
    "\n",
    "        # in / out feature maps at each scale\n",
    "        self.scale_3_in, self.scale_3_out = in_channels, 32\n",
    "        self.scale_2_in, self.scale_2_out = 32, 32\n",
    "        self.scale_1_in, self.scale_1_out = 32, 64\n",
    "\n",
    "        # coarsest scale - outputs maps of shape B, \n",
    "        self.scale_3 = nn.Sequential(\n",
    "            nn.Conv2d(self.scale_3_in, self.scale_3_out, 4, 2), # B, 32, 31 x 31\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.scale_3_feats = nn.Sequential(\n",
    "            Flatten3D(),\n",
    "            nn.Linear(self.scale_3_out * 31 * 31, self.feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        # mid scale - outputs maps of shape B, \n",
    "        self.scale_2 = nn.Sequential(\n",
    "            nn.Conv2d(self.scale_2_in, self.scale_2_out, 4, 2), # B, 32, 14 x 14\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.scale_2_feats = nn.Sequential(\n",
    "            Flatten3D(),\n",
    "            nn.Linear(self.scale_2_out * 14 * 14, self.feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # finest scale - outs maps of shape B,\n",
    "        self.scale_1 = nn.Sequential(\n",
    "            nn.Conv2d(self.scale_1_in, self.scale_1_out, 4, 2), # B, 64, 6 x 6\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.scale_1_feats = nn.Sequential(\n",
    "            Flatten3D(),\n",
    "            nn.Linear(self.scale_1_out * 6 * 6, self.feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        self.batch_size = x.shape[0]\n",
    "        \n",
    "        scale_3_x = self.scale_3(x)\n",
    "        scale_3_feats = self.scale_3_feats(scale_3_x)\n",
    "\n",
    "        scale_2_x = self.scale_2(scale_3_x)\n",
    "        scale_2_feats = self.scale_2_feats(scale_2_x)\n",
    "\n",
    "        scale_1_x = self.scale_1(scale_2_x)\n",
    "        scale_1_feats = self.scale_1_feats(scale_1_x)\n",
    "        #print(scale_3_feats.shape)\n",
    "        #print(scale_2_feats.shape)\n",
    "        #print(scale_1_feats.shape)\n",
    "        \n",
    "        # Just stacking gives the shape (3, batch_size, feature_dim). Hence, we need to permute tp get \n",
    "        # (batch_size, feature_dim, 3)\n",
    "        multi_scale_feats = torch.stack([scale_3_feats, scale_2_feats, scale_1_feats]).permute(1,2,0)\n",
    "        \n",
    "        multi_scale_feats = multi_scale_feats.reshape(self.batch_size, self.num_nodes, 3 * self.features_to_take )\n",
    "        \n",
    "        # reshape like this so that they can be associated with each latent node\n",
    "        return multi_scale_feats\n",
    "    \n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, c_in, c_out):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(c_in, c_out)\n",
    "\n",
    "    def forward(self, node_feats, adj_matrix):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            node_feats: Tensor with node features of shape [batch_size, num_nodes, c_in]\n",
    "            adj_matrix: Batch of adjacency matrices of the graph. If there is an edge from i to j,\n",
    "                         adj_matrix[b,i,j]=1 else 0. Supports directed edges by non-symmetric matrices.\n",
    "                         Assumes to already have added the identity connections.\n",
    "                         Shape: [batch_size, num_nodes, num_nodes]\n",
    "        \"\"\"\n",
    "        # Num neighbours = number of incoming edges\n",
    "        num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)\n",
    "        # prep. msg\n",
    "        node_feats = self.projection(node_feats)\n",
    "        #print(node_feats.shape)\n",
    "        # send msg.\n",
    "        node_feats = torch.matmul(adj_matrix, node_feats)\n",
    "        node_feats = node_feats / num_neighbours\n",
    "        return node_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "66dddf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_enc = MultiScaleEncoder(4*2, 1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "eb1a9300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3083, -0.1329,  0.0334,  0.2024,  0.0683,  0.0298],\n",
      "         [-0.1563,  0.0509,  0.0678,  0.1451, -0.1596,  0.0435],\n",
      "         [-0.1692, -0.0438, -0.0202, -0.1336,  0.1160, -0.0684],\n",
      "         [ 0.0177,  0.0900, -0.0045, -0.1646,  0.1573,  0.0504]],\n",
      "\n",
      "        [[ 0.2070,  0.0479,  0.0158, -0.0309,  0.0621,  0.0077],\n",
      "         [ 0.2464,  0.0283,  0.0510, -0.3034,  0.0586,  0.0516],\n",
      "         [-0.1682,  0.0111, -0.0081,  0.2839, -0.0977, -0.0650],\n",
      "         [-0.0549,  0.0296,  0.0189,  0.1485,  0.0469,  0.0219]]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n",
      "torch.Size([2, 4, 6])\n"
     ]
    }
   ],
   "source": [
    "out = ms_enc(torch.randn(2,1,64,64))\n",
    "print(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "35d467d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0474, -0.3366],\n",
       "         [-0.0742, -0.2990],\n",
       "         [-0.1257, -0.3129],\n",
       "         [-0.1129, -0.2830]],\n",
       "\n",
       "        [[-0.0147, -0.3387],\n",
       "         [-0.0268, -0.2952],\n",
       "         [-0.0153, -0.3666],\n",
       "         [-0.0542, -0.3530]]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcn_layer = GCNLayer(c_in=6,c_out=2)\n",
    "#gcn_layer.projection.weight.data = torch.eye(6)\n",
    "#gcn_layer.projection.bias.data = torch.zeros(6)\n",
    "\n",
    "adj_mat = torch.Tensor([\n",
    "    [1., 0., 1., 1.],\n",
    "    [0., 1., 0., 1.],\n",
    "    [0., 0., 1., 0.],\n",
    "    [0., 0., 0., 1.]\n",
    "])\n",
    "\n",
    "gcn_layer(out, adj_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505c65f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0c1e42d09abd9a7c7967c6caf04ac65128c9e19aa5355439d6e17d7b03582b55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
