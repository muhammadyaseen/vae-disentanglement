{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92a3dec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Saarbrucken\\EDA_Research\\vae-disentanglement\\disentanglement_lib_pl\n"
     ]
    }
   ],
   "source": [
    "cd D:\\Saarbrucken\\EDA_Research\\vae-disentanglement\\disentanglement_lib_pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c54c28bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# genernal\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from architectures import encoders, decoders\n",
    "from common.ops import Flatten3D, Unsqueeze3D, Reshape\n",
    "from torch.nn import functional as F\n",
    "import pickle\n",
    "from common import dag_utils\n",
    "\n",
    "# model loading\n",
    "from gnncsvae_experiment import GNNCSVAEExperiment\n",
    "from collections import defaultdict, namedtuple\n",
    "import models\n",
    "from common.notebook_utils import get_configured_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# for plotting stuff\n",
    "from matplotlib import cm as mpl_colormaps\n",
    "from common.utils import CenteredNorm\n",
    "import matplotlib.pyplot as plt \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7568203f",
   "metadata": {},
   "source": [
    "# Multiscale + GNN structure test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eca36f",
   "metadata": {},
   "source": [
    "## Testing Prior GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbe25a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from common.special_modules import SimpleGNNLayer\n",
    "\n",
    "# V, ifd, ofd =5, 2, 4\n",
    "\n",
    "# #inp = torch.randn(size=(1, V, ifd))\n",
    "\n",
    "# inp = torch.arange(10).type(torch.FloatTensor).view(1, V, ifd)\n",
    "# print(inp)\n",
    "# print(\"input shape: \", inp.shape)\n",
    "\n",
    "# prior_gnn = SimpleGNNLayer(ifd, ofd, A.T, is_final_layer=True)\n",
    "# print(\"Linear layer mat shape: \", prior_gnn.projection.weight.data.shape)\n",
    "# prior_gnn.projection.weight.data = torch.Tensor(\n",
    "#         [[1., 0.],\n",
    "#         [0., 1.],\n",
    "#         [1., 0.],\n",
    "#         [0., 1.]]\n",
    "# )\n",
    "# prior_gnn.projection.bias.data = torch.zeros(ofd)\n",
    "# print(\"input: \", inp)\n",
    "# out = prior_gnn(inp)\n",
    "# print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef802fef",
   "metadata": {},
   "source": [
    "# Load model and test prior components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "695e1ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ModelParams = namedtuple('ModelParams', [\"z_dim\", \"l_dim\", \"num_labels\" , \"in_channels\", \n",
    "                                        \"image_size\", \"batch_size\", \"w_recon\", \"w_kld\", \"kl_warmup_epochs\",\n",
    "                                         \"adjacency_matrix\"])\n",
    "\n",
    "\n",
    "algo_name = \"GNNBasedConceptStructuredVAE\"\n",
    "checkpoint_path = r\"D:\\Saarbrucken\\EDA_Research\\vae-disentanglement\\models\\gnncsvae.ckpt\"\n",
    "z_dim = 5\n",
    "\n",
    "model_params = ModelParams(\n",
    "        [z_dim], 6, 0, 1, 64, 64, 1.0, 1.0, 0,\n",
    "    r\"D:\\Saarbrucken\\EDA_Research\\vae-disentanglement\\adjacency_matrices\\dsprites_correlated.pkl\"\n",
    ")\n",
    "exp_params = dict(\n",
    "        in_channels=1,\n",
    "        image_size=64,\n",
    "        LR=1e-4,\n",
    "        weight_decay=0.0,       \n",
    "        dataset=\"dsprites_correlated\",\n",
    "        datapath=r\"D:\\Saarbrucken\\EDA_Research\\vae-disentanglement\\datasets\",\n",
    "        droplast=True,        \n",
    "        batch_size=64,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        seed=123,\n",
    "        evaluation_metrics=None,\n",
    "        visdom_on=False,\n",
    "        save_dir=None,\n",
    "        max_epochs=1,\n",
    "        l_zero_reg=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729ce918",
   "metadata": {},
   "source": [
    "# Model with 5-dim node feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6eb544",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model_class = getattr(models, algo_name)\n",
    "vae_model = vae_model_class(model_params)\n",
    "\n",
    "vae_experiment = GNNCSVAEExperiment.load_from_checkpoint(\n",
    "            checkpoint_path,\n",
    "            vae_model=vae_model, \n",
    "            params=exp_params,\n",
    "            dataset_params=dict(correlation_strength=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e8a4741",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize [CorrelatedDSpritesDataset] with 737280 examples. Shape (737280, 64, 64).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.environ['DISENTANGLEMENT_LIB_DATA'] = r\"D:\\Saarbrucken\\EDA_Research\\vae-disentanglement\\datasets\"\n",
    "\n",
    "dataset = get_configured_dataset(\"dsprites_correlated\")\n",
    "sample_loader = DataLoader(dataset, batch_size=64, shuffle = False, drop_last=True)\n",
    "\n",
    "test_input, test_label = next(iter(sample_loader))\n",
    "fwd_pass_results = vae_experiment.model.forward(test_input, current_device=test_input.device, labels = test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "411e62e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x_recon', 'prior_mu', 'prior_logvar', 'posterior_mu', 'posterior_logvar', 'latents_predicted'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fwd_pass_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a9e8e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2693,  0.0669,  0.7291, -0.0036,  1.4615],\n",
       "        [ 0.2693,  0.0670,  0.7291, -0.0036,  1.4615],\n",
       "        [ 0.2693,  0.0670,  0.7291, -0.0036,  1.4614],\n",
       "        [ 0.2693,  0.0670,  0.7291, -0.0036,  1.4614],\n",
       "        [ 0.2693,  0.0670,  0.7291, -0.0036,  1.4614]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fwd_pass_results['prior_mu'][1,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053b8051",
   "metadata": {},
   "source": [
    "## Check projection matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5e9706e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 29)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x28292872160>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Ws = [vae_experiment.model.prior_gnn[i].projection.weight.detach() for i in range(3)]\n",
    "Ws_to_plot = torch.cat(Ws, dim = 1).numpy()\n",
    "print(Ws_to_plot.shape)\n",
    "fig, axs = plt.subplots(figsize=(10, 6))\n",
    "fig.tight_layout(pad=0)\n",
    "axs.margins(0)\n",
    "plt.axis('off')\n",
    "plt.imshow(Ws_to_plot, cmap=mpl_colormaps.coolwarm, norm=CenteredNorm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "691d0b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 29)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x282928b6e50>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Ws = [vae_experiment.model.encoder_gnn[i].projection.weight.detach() for i in range(3)]\n",
    "Ws_to_plot = torch.cat(Ws, dim = 1).numpy()\n",
    "print(Ws_to_plot.shape)\n",
    "fig, axs = plt.subplots(figsize=(10, 6))\n",
    "fig.tight_layout(pad=0)\n",
    "axs.margins(0)\n",
    "plt.axis('off')\n",
    "plt.imshow(Ws_to_plot, cmap=mpl_colormaps.coolwarm, norm=CenteredNorm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ff640b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([10, 9]), torch.Size([10, 10]), torch.Size([10, 10])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " [vae_experiment.model.prior_gnn[i].projection.weight.size() for i in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ca27d7",
   "metadata": {},
   "source": [
    "# For the 2D node feat dim model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "684b5881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.]])\n",
      "GNNBasedConceptStructuredVAE Model Initialized\n"
     ]
    }
   ],
   "source": [
    "algo_name = \"GNNBasedConceptStructuredVAE\"\n",
    "checkpoint_path = r\"D:\\Saarbrucken\\EDA_Research\\vae-disentanglement\\models\\gnncsvae2d.ckpt\"\n",
    "z_dim = 2\n",
    "model_params = ModelParams(\n",
    "        [z_dim], 6, 0, 1, 64, 64, 1.0, 1.0, 0,\n",
    "    r\"D:\\Saarbrucken\\EDA_Research\\vae-disentanglement\\adjacency_matrices\\dsprites_correlated.pkl\"\n",
    ")\n",
    "vae_model_class = getattr(models, algo_name)\n",
    "vae_model = vae_model_class(model_params)\n",
    "\n",
    "vae_experiment = GNNCSVAEExperiment.load_from_checkpoint(\n",
    "            checkpoint_path,\n",
    "            vae_model=vae_model, \n",
    "            params=exp_params,\n",
    "            dataset_params=dict(correlation_strength=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfefe65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 17)\n",
      "(4, 17)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2225dc8f310>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Ws = [vae_experiment.model.prior_gnn[i].projection.weight.detach() for i in range(3)]\n",
    "Ws_to_plot = torch.cat(Ws, dim = 1).numpy()\n",
    "print(Ws_to_plot.shape)\n",
    "fig, axs = plt.subplots(figsize=(10, 6))\n",
    "fig.tight_layout(pad=0)\n",
    "axs.margins(0)\n",
    "plt.axis('off')\n",
    "plt.imshow(Ws_to_plot, cmap=mpl_colormaps.coolwarm, norm=CenteredNorm())\n",
    "\n",
    "Ws = [vae_experiment.model.encoder_gnn[i].projection.weight.detach() for i in range(3)]\n",
    "Ws_to_plot = torch.cat(Ws, dim = 1).numpy()\n",
    "print(Ws_to_plot.shape)\n",
    "fig, axs = plt.subplots(figsize=(10, 6))\n",
    "fig.tight_layout(pad=0)\n",
    "axs.margins(0)\n",
    "plt.axis('off')\n",
    "plt.imshow(Ws_to_plot, cmap=mpl_colormaps.coolwarm, norm=CenteredNorm())\n",
    "\n",
    "# Red==positive, Blue==negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "679601ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0663, -0.0145,  0.0393, -0.0600, -0.0453,  0.0603,  0.0362, -0.0366,\n",
       "          0.0034],\n",
       "        [ 0.0825, -0.0195,  0.0502, -0.0784, -0.0591,  0.0795,  0.0479, -0.0504,\n",
       "          0.0041],\n",
       "        [ 0.0316, -0.0059,  0.0186, -0.0286, -0.0209,  0.0289,  0.0167, -0.0171,\n",
       "          0.0021],\n",
       "        [ 0.0882, -0.0196,  0.0523, -0.0808, -0.0611,  0.0812,  0.0480, -0.0512,\n",
       "          0.0043]], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_experiment.model.prior_gnn[0].projection.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2860bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.5789,  0.6704,  0.5819, -0.1446],\n",
       "        [-0.3088, -0.1422,  0.3604,  0.2173],\n",
       "        [-0.4159,  1.0702,  0.5456, -0.4264],\n",
       "        [-0.3753,  0.2776,  1.3673, -0.1371]], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_experiment.model.prior_gnn[1].projection.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42161b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0908,  0.1481, -0.0526,  0.4951],\n",
       "        [-1.6003, -1.2836, -1.1983, -0.9343],\n",
       "        [ 0.1036, -1.0648,  0.0742,  1.5846],\n",
       "        [ 1.8579, -1.8139, -1.3088,  1.8840]], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_experiment.model.encoder_gnn[2].projection.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc75b99b",
   "metadata": {},
   "source": [
    "prior_gnn_out = None\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    exogen_samples = torch.randn(size=(1, vae_experiment.model.num_nodes, vae_experiment.model.encoder_cnn.out_feature_dim))   \n",
    "    prior_gnn_out = vae_experiment.model.prior_gnn(exogen_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9739e548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4093, -0.7022, -0.5794,  0.4828],\n",
       "         [ 0.4093, -0.7076, -0.5766,  0.4833],\n",
       "         [ 0.4480, -0.6749, -0.5656,  0.5306],\n",
       "         [ 0.2955, -0.7814, -0.6187,  0.3378],\n",
       "         [ 0.3965, -0.7163, -0.5839,  0.4648]],\n",
       "\n",
       "        [[ 0.4339, -0.6893, -0.5708,  0.5119],\n",
       "         [ 0.3732, -0.7306, -0.5929,  0.4403],\n",
       "         [ 0.5568, -0.5555, -0.5158,  0.6565],\n",
       "         [ 0.4293, -0.6876, -0.5723,  0.5080],\n",
       "         [ 0.2707, -0.7933, -0.6268,  0.3073]]], grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# already after the first projection, values are very similar\n",
    "vae_experiment.model.prior_gnn[0](exogen_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78350c23",
   "metadata": {},
   "source": [
    "# Sup Reg test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57e3a229",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedRegulariser(nn.Module):\n",
    "\n",
    "    def __init__(self, num_nodes, node_features_dim, node_type_map):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.node_features_dim = node_features_dim\n",
    "        self.supervised_regularisers = nn.ModuleList([nn.Linear(self.node_features_dim, 1) for n in range(self.num_nodes)])\n",
    "\n",
    "    def forward(self, node_features):\n",
    "        \"\"\"\"\n",
    "        node_features has shape (batch, V, feature_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        # We need to break it into V different vectors\n",
    "        # i.e. a list L which has elements of shape (batch, feature_dim) and len(L) = V\n",
    "        node_features_separated = node_features.chunk(self.num_nodes, dim=1)\n",
    "\n",
    "        # now we have a list where node_features_separate[i] gives features associated \n",
    "        # with i-th node. But the current output shape would be (batch, 1, feature_dim)\n",
    "        # so we have to squeeze out the singleton dim\n",
    "\n",
    "        predictions_all_nodes = []\n",
    "        for node_idx in range(self.num_nodes):\n",
    "            \n",
    "            # convert (batch, 1, feature_dim) to (batch, feature_dim)\n",
    "            features_of_this_node = node_features_separated[node_idx].squeeze(dim=1)\n",
    "\n",
    "            predictions_all_nodes.append(self.supervised_regularisers[node_idx](features_of_this_node))\n",
    "\n",
    "        return predictions_all_nodes\n",
    "        \n",
    "\n",
    "    def loss(self, predictions_all_nodes, targets_all_nodes):\n",
    "        \"\"\"\n",
    "        predictions_all_nodes: List of length `self.num_nodes`, where each element i of the list corresponds to the batch of predictions associated with i-th node \n",
    "        targets_all_nodes: Similar as above\n",
    "        \"\"\"\n",
    "        loss_per_node = dict()\n",
    "        total_loss = 0.0\n",
    "        targets_all_nodes = targets_all_nodes.chunk(self.num_nodes, dim=1)\n",
    "        for node_idx in range(self.num_nodes):\n",
    "\n",
    "            #loss_this_node = self.get_loss(node_idx)(predictions_all_nodes[node_idx], targets_all_nodes[node_idx]).detach()\n",
    "            loss_this_node = F.mse_loss(predictions_all_nodes[node_idx], targets_all_nodes[node_idx])\n",
    "            # Does it make sense to sum it? they're different types of losses and have different units etc\n",
    "            total_loss += loss_this_node\n",
    "            loss_per_node[f'Node_{node_idx}'] = loss_this_node.detach()\n",
    "\n",
    "        return total_loss, loss_per_node\n",
    "\n",
    "    def get_loss(self, node_idx):\n",
    "        raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "439d8e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_reg = SupervisedRegulariser(5, 2, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "751da29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = sup_reg(\n",
    "    torch.randn(2, 5, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa05ded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.randn(2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1569be93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.6404, grad_fn=<AddBackward0>),\n",
       " {'Node_0': tensor(0.5651),\n",
       "  'Node_1': tensor(2.6159),\n",
       "  'Node_2': tensor(0.2717),\n",
       "  'Node_3': tensor(0.1565),\n",
       "  'Node_4': tensor(0.0312)})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sup_reg.loss(preds, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8a871a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.randn(2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b53348b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.3416],\n",
       "         [-0.3051]]),\n",
       " tensor([[ 0.6350],\n",
       "         [-0.1973]]),\n",
       " tensor([[1.9198],\n",
       "         [0.1163]]),\n",
       " tensor([[0.1507],\n",
       "         [0.2345]]),\n",
       " tensor([[-1.3556],\n",
       "         [-0.4662]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.chunk(5, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73346bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3416,  0.6350,  1.9198,  0.1507, -1.3556],\n",
       "        [-0.3051, -0.1973,  0.1163,  0.2345, -0.4662]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f0e8c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BifurcatedGNNLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Can be used to implement GNNs for P(Z|epsilon, A) or Q(Z|X,A)\n",
    "    \"\"\"\n",
    "    def __init__(self, adj_mat, in_ind_dim, out_ind_dim, in_dep_dim, out_dep_dim, layer_type='first'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_type = layer_type\n",
    "        self.A = adj_mat.T # this is reqd because the store mat is in from-to form but the impl needs to-from\n",
    "        \n",
    "        self.num_neighbours = self.A.sum(dim=-1, keepdims=True)\n",
    "        self.dependent_nodes_mask = self.num_neighbours - 1\n",
    "        \n",
    "        if self.layer_type == 'first':\n",
    "            self.projection_ind = nn.Linear(in_ind_dim, out_ind_dim * 2)\n",
    "            self.projection_dep = nn.Linear(in_dep_dim, out_dep_dim * 2)\n",
    "        else:\n",
    "            self.projection_ind = nn.Linear(in_ind_dim * 2, out_ind_dim * 2)\n",
    "            self.projection_dep = nn.Linear(in_dep_dim * 2, out_dep_dim * 2)\n",
    "    \n",
    "    def forward(self, node_feats):\n",
    "        \n",
    "        node_feats_dep, node_feats_ind = node_feats\n",
    "        \n",
    "        self.A = self.A.to(node_feats_dep.device)\n",
    "        self.num_neighbours = self.num_neighbours.to(node_feats_dep.device)\n",
    "        \n",
    "        if self.layer_type == 'first':\n",
    "            \n",
    "            # in the first layer we get combined features i.e. there's no bifurcation \n",
    "            # b/w dependent and independent features. Since they're coming from \n",
    "            # encoder CNN net, they're all kinda dependent\n",
    "            \n",
    "            # project to ind feat dim - prep indept features first\n",
    "            indep_node_feats = self.projection_ind(node_feats_dep)\n",
    "            indep_node_feats = torch.tanh(indep_node_feats)   \n",
    "        \n",
    "            # prep dep features\n",
    "            dep_node_feats = self.projection_dep(node_feats_dep)\n",
    "            dep_node_feats = torch.matmul(self.A, dep_node_feats)\n",
    "            dep_node_feats = dep_node_feats / self.num_neighbours\n",
    "            dep_node_feats = torch.tanh(dep_node_feats)\n",
    "            dep_node_feats = dep_node_feats * self.dependent_nodes_mask\n",
    "            \n",
    "            return dep_node_feats, indep_node_feats\n",
    "        \n",
    "        else: # if not first layer...\n",
    "            \n",
    "            # take in (dep, ind) return (dep, ind)\n",
    "            # project to ind feat dim - prep indept features first\n",
    "            indep_node_feats = self.projection_ind(node_feats_ind)\n",
    "            indep_node_feats = torch.tanh(indep_node_feats)\n",
    "\n",
    "            # prep dep features\n",
    "            dep_node_feats = self.projection_dep(node_feats_dep)\n",
    "            dep_node_feats = torch.matmul(self.A, dep_node_feats)\n",
    "            dep_node_feats = dep_node_feats / self.num_neighbours   \n",
    "            \n",
    "            if self.layer_type == 'last':\n",
    "\n",
    "                # split into mu and sigma\n",
    "                dep_node_feats = dep_node_feats * self.dependent_nodes_mask\n",
    "                dep_mu, dep_logvar = dep_node_feats.chunk(2, dim=2)\n",
    "                indep_mu, indep_logvar = indep_node_feats.chunk(2, dim=2)\n",
    "\n",
    "                # combine params\n",
    "                mu = torch.cat([indep_mu, dep_mu], dim=2)\n",
    "                logvar = torch.cat([indep_logvar, dep_logvar], dim=2)\n",
    "\n",
    "                return mu, logvar\n",
    "\n",
    "            else:\n",
    "                dep_node_feats = torch.tanh(dep_node_feats)\n",
    "                dep_node_feats = dep_node_feats * self.dependent_nodes_mask\n",
    "                return dep_node_feats, indep_node_feats\n",
    "\n",
    "    def __repr__(self):\n",
    "        \n",
    "        return self.projection.__repr__() + f\" is_final_layer={self.is_final_layer}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f597565b",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mat = torch.from_numpy(\n",
    "    np.array([\n",
    "        [1., 0., 0., 0., 0.],\n",
    "        [0., 1., 0., 0., 0.],\n",
    "        [0., 0., 1., 1., 0.],\n",
    "        [0., 0., 0., 1., 0.],\n",
    "        [0., 0., 0., 0., 1.]\n",
    "    ], dtype=np.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "73eb3534",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_net = nn.Sequential(\n",
    "    BifurcatedGNNLayer(adj_mat, 6, 2, 6, 1, 'first'),\n",
    "    BifurcatedGNNLayer(adj_mat, 2, 2, 1, 1, 'last')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d4c0e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exogen_vars = torch.randn(size=(1,5,6))\n",
    "mu, logvar = gnn_net((exogen_vars, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f025bf39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1105, -0.1747, -0.0000],\n",
       "         [-0.1205,  0.4166, -0.0000],\n",
       "         [ 0.0602,  0.0829, -0.0000],\n",
       "         [ 0.1033, -0.1220, -0.1386],\n",
       "         [-0.2060, -0.1597, -0.0000]]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4797e23a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4286, -0.1815, -0.0000],\n",
       "         [-0.1640,  0.2369, -0.0000],\n",
       "         [-0.2850, -0.1368, -0.0000],\n",
       "         [-0.4309,  0.0548, -0.8954],\n",
       "         [-0.2719, -0.3519, -0.0000]]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842d4013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0c1e42d09abd9a7c7967c6caf04ac65128c9e19aa5355439d6e17d7b03582b55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
