{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current VAEs and Objective\n",
    "\n",
    "**Current VAEs methods can be divided into two categories depending on how they deal will correlated latents**:\n",
    "\n",
    "1.**(Full)** Methods that don't care about correlations at all and make no attempt at resolving them. Vanilla VAE with single latent layer and Hierarchichal VAEs like LadderVAE, BIVA etc. are examples of this class. Usually use The use fully conditioned priors.\n",
    "\n",
    "2.**(None)** Methods that deal with mild or synthetically constructed correlations and attempt to resolve all the correlations entirely, mostly because of fairness reasons. and do it post-hoc. Ada-Beta-VAE is an example of this class which relies on Beta-VAE based methods. The use independent priors.\n",
    "\n",
    "**Some observations**:\n",
    "\n",
    "- In real world, latent structure is not fully connected. Latent interactions are sparse and more meaningful\n",
    "- Even with fairness in mind, not all correlations are harmful. We might want to resolve *some* correlations (e.g. for senstitive attributes) and *preserve* some other correlations (imposed by nature, environment etc.)\n",
    "\n",
    "**We care about two things:**\n",
    "1. Sparse and meaningful/interpretable latent interactions\n",
    "2. Possibility to remove unwanted correlations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Example to Illustrate\n",
    "\n",
    "**As an Example, assume the following true latent structure**:\n",
    "<img src=\"norm_dag.jpg\" alt=\"Latent Structure\" width=\"300\" height=\"300\"/>\n",
    "\n",
    "Further assume that we regard (gender, height) pair as sensitive and would want to learn a latent structure which doesn't have this association (as shown below):\n",
    "<img src=\"marg_dag.jpg\" alt=\"Latent Structure\" width=\"300\" height=\"300\"/>\n",
    "\n",
    "**Such multi-layered latent structure can be specified by hierarchical VAEs e.g. Ladder VAE**\n",
    "\n",
    "**But they have a serious limitation w.r.t. our objective**. The latent hierarchy is fully-conditional i.e every dimension of layer 1 is dependent on all the dimensions in layer 2. That is, the interactions are **not sparse and not meaningful**.\n",
    "\n",
    "In math:\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\textbf{z}_1 | \\textbf{z}_2) &= p( z_1^1, z_1^2, z_1^3 |  z_2^1, z_2^2, z_2^3) \\\\\n",
    "&= p(z_1 | z_2^1, z_2^2, z_2^3) \\times p(z_1 | z_2^1, z_2^2, z_2^3) \\times p(z_1 | z_2^1, z_2^2, z_2^3)\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "As seen in the example, we want more structured dependencies than this. An example of what we want would be this:\n",
    "\n",
    "Let,\n",
    "\n",
    "  \\begin{align}\n",
    "    \\textbf{z}_2 = \\begin{bmatrix}\n",
    "           age \\\\\n",
    "           gender\n",
    "         \\end{bmatrix} \\quad\n",
    "    \\textbf{z}_1 = \\begin{bmatrix}\n",
    "           edu level \\\\\n",
    "           height \\\\\n",
    "           hair length\n",
    "         \\end{bmatrix} \n",
    "\\end{align}\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\textbf{z}_1 | \\textbf{z}_2) &= p(\\texttt{age}) \\times  p(\\texttt{gender}) \\\\ &\\times p(\\texttt{edu level} | \\texttt{age}) \\times p(\\texttt{hair len} | \\texttt{gender}) \\times p(\\texttt{height} | \\texttt{age, gender})\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to impose such structure?\n",
    "\n",
    "For this we need :\n",
    "1. **A VAE which allows layered latents** \n",
    "2. **Structured Prior** and  \n",
    "3. **Structured Variational Distributions**\n",
    "\n",
    "**Ans 1 - We can adapt Ladder VAE to represent layered latents**\n",
    "\n",
    "**Ans 2 and 3 - How to find the prior structure and How to impose that structure in the variational distributions?** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Structure\n",
    "\n",
    "For this we have two approaches \n",
    "- (1) Max likelihood Bayesian Structure estimation via Chow-Liu algo\n",
    "- (2) K Active Parents via $l_0$ regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Max likelihood Bayesian Structure estimation via Chow-Liu algo\n",
    "\n",
    "**Given** : Latent Labels\n",
    "\n",
    "**Assumption** : Latent structure is a tree (in case of trees, globally optimal (i.e. MLE) graph structure can be found via exact methods)\n",
    "\n",
    "It can be extended to > 2 layers as long as the latent structure remains a tree. See the following:\n",
    "\n",
    "<img src=\"tree.jpg\" alt=\"Latent Structure\" width=\"300\" height=\"300\"/>\n",
    "\n",
    "\n",
    "**Goal**: Find latent structure and impose it on prior and variational dists...\n",
    "\n",
    "Boils down to (Ch26, Kevin Murphy):\n",
    "- Maximising the following objective (for bernoulli and categorial we can get distributions simply by counting)\n",
    "\n",
    "$$ p(\\textbf{z}) = \\prod_v p(z_v) \\times \\prod_{(s,t)} \\frac{p(z_s,z_t)}{p(z_s)\\times p(z_t)} $$\n",
    "\n",
    "- Compute maximum weight spanning tree where edge weights are pairwise MI computed from empirical distribution.\n",
    "\n",
    "\n",
    "\n",
    "- **This pre-processing steps gives us a graph topology that can be represented as an adjacency matrix**. The topology and the matrix is symmetric. If we want to use this topology in a prior we need to convert it into a DAG adj. mat.\n",
    "\n",
    "- If the number of latent variables in low we can do so by inspection and assign each latent to higher or lower layer as we like.\n",
    "\n",
    "- If the latents are too many, we can use simple heuristics to create two groups. One example is to assign a variable to higher layer if it has a higher than avg degree as it means it is involved in many latent variables and likely to be independent itself.\n",
    "\n",
    "- Once we have established a topology either by inspection or heuristics we can construct prior and posterior distributions which respect that topology.\n",
    "\n",
    "\n",
    "As example assume that we have grouped the variables as such:\n",
    "  \\begin{align}\n",
    "    \\textbf{z}_1 = \\begin{bmatrix}\n",
    "           age \\\\\n",
    "           gender\n",
    "         \\end{bmatrix} \\quad\n",
    "    \\textbf{z}_2 = \\begin{bmatrix}\n",
    "           edu level \\\\\n",
    "           height \\\\\n",
    "           hair length\n",
    "         \\end{bmatrix} \n",
    "\\end{align}\n",
    "  \n",
    "And the structure found is this (without the X):\n",
    "\n",
    "<img src=\"norm_dag.jpg\" alt=\"Latent Structure\" width=\"300\" height=\"300\"/>\n",
    "\n",
    "\n",
    "We modify ladder-vae as follows:\n",
    "\n",
    "The top layer is represented as isotropic Gaussians.\n",
    "\n",
    "**The parameters for lower layer are produced by an NN which respects the learned topology.**\n",
    "\n",
    "$p(\\textbf{z}_1 | \\textbf{z}_2)$ and $q(\\textbf{z}_1 | \\textbf{z}_2, \\textbf{x})$ is modelled by such NNs.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"nnex.jpg\" alt=\"Latent Structure\" width=\"300\" height=\"300\"/>\n",
    "\n",
    "Or more generally:\n",
    "<img src=\"specialnet.jpg\" alt=\"Latent Structure\" width=\"300\" height=\"300\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) K active parents\n",
    "\n",
    "- This approach doesn't require any latent labels. \n",
    "- This approach doesn't assume latents to have a tree structure. \n",
    "- This approach doesn't allow us to delete unwanted correlations\n",
    "\n",
    "Idea behind K-active-parents is that each output unit is only allowed to depend on a maximum of k input units. \n",
    "\n",
    "<img src=\"kactive.jpg\" alt=\"Latent Structure\" width=\"600\" height=\"300\"/>\n",
    "\n",
    "**Note that if we remove the violet edge, we no longer have a tree (rather a forest)**\n",
    "\n",
    "\n",
    "If we have A input units and B output units with an intermediate hidden layer we can use the NN structure we saw in previous approach:\n",
    "\n",
    "<img src=\"specialnet.jpg\" alt=\"Latent Structure\" width=\"300\" height=\"300\"/>\n",
    "\n",
    "If the number of units in the box is $r$, then each box can have a maximum of $r \\times k$ incoming non-zero weights.\n",
    "\n",
    "**Regularizer**: We can penalize this using $l_0$ norm but it is not SGD-optimizable since it is not continuous. Thus to optimize this we need to use the surrogate $l_0$ loss introduced in Learning Sparse NN paper (Kigma and Welling, ICLR 2018).\n",
    "\n",
    "Key idea: Multiply the weights with bernoulli random variables then minimize the \"on\" probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
