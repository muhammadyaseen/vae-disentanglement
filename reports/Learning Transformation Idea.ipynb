{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ab5eff1",
   "metadata": {},
   "source": [
    "In paired $(\\textbf{x}_1,\\textbf{x}_2)$ setting:\n",
    "\n",
    " * Can we consider learning transformation b/w pairs a generalization of this idea ?\n",
    " * Can we get rid of explicit pairs somehow and train directly on single example case \n",
    " * Transformation complexity acting as a regularizer\n",
    " * What transformation? How to parameterize and where to enfore them in network?\n",
    " * Connection to latent semantic vector math?\n",
    "\n",
    "If each latent unit is responsible for only a single FoV, then only a limited number of ‘connections’ / ‘activations’ should be fired for transforming each pair. If the total number of latents is $d$ and $k$ factors are shared then we only need to activate $d - k$ units (ideally) to transform $x_1$ into $x_2$. This information can be used as a regularizer. I need to think if it makes sense to use the formalism of block-diagonal matrices for this. If not, what other form of regularization can be used? (“Learning disent structure of dyn envs” and “Cfactuals uncover Modular structure of DNNs” papers could be relevant here?) \n",
    "\n",
    "\n",
    "**What is meant by shared factors?**\n",
    "\n",
    "For example in the case when the domain of factors vary smoothly as opposed to discrete value in synthetic datasets e.g in MNIST. How would we *find* two images which have same `thickness` or `rotation` value? These are vague / approximate notions so we probably need a notion of “factors are approximately shared” or “close enough in latent space”.\n",
    "\n",
    "**Nature of factor might affect structure of latent encoding?**\n",
    "\n",
    " The nature of latent factor determines how its values affect the reconstruction (and hence loss), and how encoding it in latent space can be different. For example, `posX = 0` and `posX = 39` are at opposite ends of the image so the loss will be max (\\*), but in case of orientation there exists several ‘aliased’ values for which reconstruction loss is very close to zero even if the latent value itself is far apart e.g. rotation = 0 deg looks very close to rotation = 350 degree, even if the values are at extremes of range. In case of a square `rotation` $= 0,90,270,360$ all result in the “same” image because of symmetry. Cartesian coords aren't the most natural system for some latents i.e. orthonormal vecs aren't the most natural basis (How to combine spaces representated in different corrdinates?)\n",
    "\n",
    "How will / should such numerically different but semantically close values be encoded in the latent space ? \n",
    "\n",
    "(\\*) It actually won't be max. Once we have zero overlap with the true image, all loss values are the same at least for `pos` variables. So in a way, how far we are from true position doesn't matter and isn't used as a signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea5b4b8",
   "metadata": {},
   "source": [
    "## Training (what we want?)\n",
    "\n",
    "<img src=\"vec_image.jpg\" alt=\"drawing\" width=\"300\" height=\"300\"/>\n",
    "\n",
    "1. Take an example $\\textbf{x}_1$ and encode it to get $\\textbf{z}_1$. Pick another example $\\textbf{x}_2$ and encode it to get $\\textbf{z}_2$\n",
    "3. Have a function $f_\\psi$ which takes as input $\\textbf{z}_1$ and \"transforms\" it into $\\textbf{z}_2$ i.e. $ \\hat{\\textbf{z}}_2 = f_\\psi(\\textbf{z}_1)$\n",
    "4. This function should have limited modeling capacity\n",
    "5. $\\hat{\\textbf{z}}_2$ should be able to recontruct $\\textbf{x}_2$, the function $f_\\psi$ then represents a transformation in latent space\n",
    "6. Let the total number of latent factors be $L$. Then $f_\\psi$ will act on only those dimensions(corresponding to FoV) which change value b/w  $\\textbf{x}_1$  and $\\textbf{x}_2$. The strength of action should depend on the difference in FoV values. It means that transformation can't be fixed, it has to depend on the pair $(\\textbf{x}_1,\\textbf{x}_2)$. Naturally we'd expect closer pair to require 'less' transformation. Would we have $L$ such transformations? \n",
    "7. In early stages of training, encodings would be crap. So learning transformations at early stages isn't probably a good thing?\n",
    "8. Would it be an invertible one-to-one transformation? (Normalizing flows helpful?)\n",
    "\n",
    "It seems that in any case we'll need to do 2 passes thru decoder. I'm not sure if backprop after 2 passes makes sense in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81736b45",
   "metadata": {},
   "source": [
    "### Sketch objective?\n",
    "\n",
    "\\begin{aligned}\n",
    "\\max_{\\phi,\\theta, \\psi} \\mathbb{E}_{(\\textbf{x}_1,\\textbf{x}_2)}  & \\bigg\\{\n",
    " \\mathbb{E}_{q_\\phi(\\textbf{z}|\\textbf{x}_1)} \\log(p_\\theta(\\textbf{x}_1|{\\textbf{z}}))  \\\\\n",
    "&+ \\mathbb{E}_{q_\\phi(\\textbf{z}|\\textbf{x}_2)} \\log(p_\\theta(\\textbf{x}_2|{\\textbf{z}})) \\\\\n",
    "&- \\beta  D_{KL}( q_\\phi({\\textbf{z}}|\\textbf{x}_1)||p({\\textbf{z}})) \\\\\n",
    "&- \\beta  D_{KL}( q_\\phi({\\textbf{z}}|\\textbf{x}_2)||p({\\textbf{z}})) \\\\\n",
    "&+ \\gamma \\text{Recon(Dec(} f_\\psi(\\textbf{z}_1,\\textbf{z}_2)) - \\textbf{x}_2)\n",
    "\\bigg\\}\n",
    "\\end{aligned}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9c6686",
   "metadata": {},
   "source": [
    "## A Gaussian Process diversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0e3fde",
   "metadata": {},
   "source": [
    "1. One of the key themes of above idea is to view the dataset as a whole. That is to view each data point $\\textbf{x}$ as being related to every other datapoint $\\hat{\\textbf{x}}$ via some transformation.\n",
    "\n",
    "2. GPs very naturally encode this idea in their covariance function $k(\\textbf{x},\\hat{\\textbf{x}})$.\n",
    "\n",
    "3. Since we have multiple axes-parallel transformations, we would need a combination / library of transformations to change one datapoint into another. This gives an idea of somehow learning these via kernels and then learning to combine them.\n",
    "\n",
    "4. We already model the latent $\\textbf{z}$ as a gaussian. We now have to model the dynamics in Z-space as composed of GPs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3ab051",
   "metadata": {},
   "source": [
    "### 3d Shapes data\n",
    "\n",
    "https://github.com/deepmind/3d-shapes\n",
    "\n",
    "We have factors floor color, wall color, object color, scale, shape, rotation\n",
    "\n",
    "We can consider 3 objects in the scene (1) Object (2) Floor (3) Wall\n",
    "\n",
    "Rotation affects appearance of all the objects\n",
    "\n",
    "ToDo: Check Google's post on explainable visual classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95aca13",
   "metadata": {},
   "source": [
    " ### Disentanglement principle re-thinking\n",
    " \n",
    "- We can't assume to know the number of concepts\n",
    "- We may have some idea about the nature of some concepts, but not all\n",
    "- We might need more than one-dim to represent some concepts\n",
    "- What would be an unsupervised way to check for disentanglement? When we don't have labels like in MNIST?\n",
    "- Some concepts might apply to whole image or a bigger region of image and some might be local e.g. background color vs eye-color\n",
    "- Different concepts may be learned at different layer depths (i.e. simple vs complex) \n",
    "- concepts learned at different different iteration\n",
    "- Correlated latents?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fda582",
   "metadata": {},
   "source": [
    "Kate and Jonas' idea seems to require FC nets so that they can enforce the block-diagonal structure.\n",
    "\n",
    "Can we do it in CNN somehow?\n",
    "\n",
    "In CNNs a filter runs through whole image to make an activation map. It collects information globally\n",
    "\n",
    "Building blocks of CNN and how they might affect information flow\n",
    "1. Kernels and biases\n",
    "2. Activation functions\n",
    "3. Pooling\n",
    "4. Later FC layers\n",
    "\n",
    "When we use CNN in reconstruction configuration, does it \"store\" the state (i.e. value) and position of each pixel somewhere in the network so that it can be reconstructed? That feels strange. \n",
    "When reconstruction what operations in a CNN allow 1-d numbers to be transformed into an image. We only have few basic ops like Decov, ReLU, pool, addition, scaling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0ce5d7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
