{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ab5eff1",
   "metadata": {},
   "source": [
    "In paired $(\\textbf{x}_1,\\textbf{x}_2)$ setting:\n",
    "\n",
    " * Can we consider learning transformation b/w pairs a generalization of this idea ?\n",
    " * Can we get rid of explicit pairs somehow and train directly on single example case \n",
    " * Transformation complexity acting as a regularizer\n",
    " * What transformation? How to parameterize and where to enfore them in network?\n",
    " * Connection to latent semantic vector math?\n",
    "\n",
    "If each latent unit is responsible for only a single FoV, then only a limited number of ‘connections’ / ‘activations’ should be fired for transforming each pair. If the total number of latents is $d$ and $k$ factors are shared then we only need to activate $d - k$ units (ideally) to transform $x_1$ into $x_2$. This information can be used as a regularizer. I need to think if it makes sense to use the formalism of block-diagonal matrices for this. If not, what other form of regularization can be used? (“Learning disent structure of dyn envs” and “Cfactuals uncover Modular structure of DNNs” papers could be relevant here?) \n",
    "\n",
    "\n",
    "**What is meant by shared factors?**\n",
    "\n",
    "For example in the case when the domain of factors vary smoothly as opposed to discrete value in synthetic datasets e.g in MNIST. How would we *find* two images which have same `thickness` or `rotation` value? These are vague / approximate notions so we probably need a notion of “factors are approximately shared” or “close enough in latent space”.\n",
    "\n",
    "**Nature of factor might affect structure of latent encoding?**\n",
    "\n",
    " The nature of latent factor determines how its values affect the reconstruction (and hence loss), and how encoding it in latent space can be different. For example, `posX = 0` and `posX = 39` are at opposite ends of the image so the loss will be max (\\*), but in case of orientation there exists several ‘aliased’ values for which reconstruction loss is very close to zero even if the latent value itself is far apart e.g. rotation = 0 deg looks very close to rotation = 350 degree, even if the values are at extremes of range. In case of a square `rotation` $= 0,90,270,360$ all result in the “same” image because of symmetry. Cartesian coords aren't the most natural system for some latents i.e. orthonormal vecs aren't the most natural basis (How to combine spaces representated in different corrdinates?)\n",
    "\n",
    "How will / should such numerically different but semantically close values be encoded in the latent space ? \n",
    "\n",
    "(\\*) It actually won't be max. Once we have zero overlap with the true image, all loss values are the same at least for `pos` variables. So in a way, how far we are from true position doesn't matter and isn't used as a signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea5b4b8",
   "metadata": {},
   "source": [
    "## Training (what we want?)\n",
    "\n",
    "<img src=\"vec_image.jpg\" alt=\"drawing\" width=\"300\" height=\"300\"/>\n",
    "\n",
    "1. Take an example $\\textbf{x}_1$ and encode it to get $\\textbf{z}_1$. Pick another example $\\textbf{x}_2$ and encode it to get $\\textbf{z}_2$\n",
    "3. Have a function $f_\\psi$ which takes as input $\\textbf{z}_1$ and \"transforms\" it into $\\textbf{z}_2$ i.e. $ \\hat{\\textbf{z}}_2 = f_\\psi(\\textbf{z}_1)$\n",
    "4. This function should have limited modeling capacity\n",
    "5. $\\hat{\\textbf{z}}_2$ should be able to recontruct $\\textbf{x}_2$, the function $f_\\psi$ then represents a transformation in latent space\n",
    "6. Let the total number of latent factors be $L$. Then $f_\\psi$ will act on only those dimensions(corresponding to FoV) which change value b/w  $\\textbf{x}_1$  and $\\textbf{x}_2$. The strength of action should depend on the difference in FoV values. It means that transformation can't be fixed, it has to depend on the pair $(\\textbf{x}_1,\\textbf{x}_2)$. Naturally we'd expect closer pair to require 'less' transformation. Would we have $L$ such transformations? \n",
    "7. In early stages of training, encodings would be crap. So learning transformations at early stages isn't probably a good thing?\n",
    "8. Would it be an invertible one-to-one transformation? (Normalizing flows helpful?)\n",
    "\n",
    "It seems that in any case we'll need to do 2 passes thru decoder. I'm not sure if backprop after 2 passes makes sense in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81736b45",
   "metadata": {},
   "source": [
    "### Sketch objective?\n",
    "\n",
    "\\begin{aligned}\n",
    "\\max_{\\phi,\\theta, \\psi} \\mathbb{E}_{(\\textbf{x}_1,\\textbf{x}_2)}  & \\bigg\\{\n",
    " \\mathbb{E}_{q_\\phi(\\textbf{z}|\\textbf{x}_1)} \\log(p_\\theta(\\textbf{x}_1|{\\textbf{z}}))  \\\\\n",
    "&+ \\mathbb{E}_{q_\\phi(\\textbf{z}|\\textbf{x}_2)} \\log(p_\\theta(\\textbf{x}_2|{\\textbf{z}})) \\\\\n",
    "&- \\beta  D_{KL}( q_\\phi({\\textbf{z}}|\\textbf{x}_1)||p({\\textbf{z}})) \\\\\n",
    "&- \\beta  D_{KL}( q_\\phi({\\textbf{z}}|\\textbf{x}_2)||p({\\textbf{z}})) \\\\\n",
    "&+ \\gamma \\text{Recon(Dec(} f_\\psi(\\textbf{z}_1,\\textbf{z}_2)) - \\textbf{x}_2)\n",
    "\\bigg\\}\n",
    "\\end{aligned}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9c6686",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b0e3fde",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
